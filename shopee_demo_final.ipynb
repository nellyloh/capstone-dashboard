{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import dateutil.parser as parser\n",
    "from geopy.geocoders import Nominatim\n",
    "import pycountry\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pinyin\n",
    "import sys\n",
    "from geopy.exc import GeocoderServiceError\n",
    "\n",
    "# Sentiment Analysis Packages\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name to be screened must be in English\n",
    "# Alias names can only handle Chinese characters , else return None\n",
    "def preprocess_df_to_dict(df):\n",
    "    def get_year(date):\n",
    "        try:\n",
    "            parser_obj = parser.parse(str(date))\n",
    "            return parser_obj.year\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_month(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).month\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def get_day(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).day\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "    \n",
    "    df_dict_list = df.to_dict('records')\n",
    "    cleaned_dict_list = []\n",
    "    for record in df_dict_list:\n",
    "        \n",
    "        alias = record['Alias name']\n",
    "        if alias is not None:\n",
    "            alias_is_english = isEnglish(alias)\n",
    "            if alias_is_english is False:\n",
    "                try:\n",
    "                    alias = pinyin.get(alias, format='strip', delimiter=' ')\n",
    "                except:\n",
    "                    alias = None\n",
    "        current_record = {\n",
    "            'name': record['Name to be screened'],\n",
    "            'alias' : alias,\n",
    "            'year_of_birth': get_year(record['Date of birth']),\n",
    "            'month_of_birth': get_month(record['Date of birth']),\n",
    "            'day_of_birth': get_day(record['Date of birth']),\n",
    "            'gender': record['Gender'],\n",
    "            'nationality': record['Nationality'],\n",
    "            ### delete these later on, for testing only###\n",
    "            'type_of_error': record['Type of variation (if any)'],\n",
    "            'actual_name': record['Actual name'],\n",
    "        }\n",
    "        cleaned_dict_list.append(current_record)\n",
    "    return cleaned_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ER_name_matching(name1, name2):\n",
    "    def split_name_list(name):\n",
    "        name = name.lower()\n",
    "        output = name.split(\" \")\n",
    "        return output\n",
    "\n",
    "    def preprocess_name(names_dict, word):\n",
    "        for key, value in names_dict.items():\n",
    "            if word in value:\n",
    "                return key\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def stitch_name(list1):\n",
    "        output = ''\n",
    "        for x in range(len(list1)):\n",
    "            if x==0:\n",
    "                output += list1[x]\n",
    "            else:\n",
    "                output += ' ' + list1[x]\n",
    "        return output\n",
    "\n",
    "    def phonetic_comparison(list1, list2):\n",
    "        meta_list1 = []\n",
    "        meta_list2 = []\n",
    "        nysiis_list1 = []\n",
    "        nysiis_list2 = []\n",
    "        for name_1 in list1:\n",
    "            meta_list1.append(jellyfish.metaphone(name_1))\n",
    "            nysiis_list1.append(jellyfish.nysiis(name_1))\n",
    "        for name_2 in list2:\n",
    "            meta_list2.append(jellyfish.metaphone(name_2))\n",
    "            nysiis_list2.append(jellyfish.nysiis(name_2))\n",
    "        if (set(meta_list1) == set(meta_list2)) or (set(nysiis_list1) == set(nysiis_list2)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def excel_to_dict(excel_file):\n",
    "        excel_df = pd.read_excel(excel_file)\n",
    "        excel_df.value.apply(str)\n",
    "        before_transformation = dict(zip(excel_df.key, excel_df.value))\n",
    "        dictionary = {key: [val for val in value.split(',')] for key, value in before_transformation.items()}\n",
    "        return dictionary\n",
    "            \n",
    "    names_dict = excel_to_dict('names_dict.xlsx') \n",
    "    \n",
    "    # START #\n",
    "    ### Change this if needed ###\n",
    "    threshold = 89\n",
    "    #############################\n",
    "    \n",
    "    split_list_1 = split_name_list(name1)\n",
    "    split_list_2 = split_name_list(name2) \n",
    " \n",
    "    \n",
    "    for i in range(len(split_list_1)):\n",
    "        split_list_1[i] = preprocess_name(names_dict, split_list_1[i])        \n",
    "    for i in range(len(split_list_2)):\n",
    "        split_list_2[i] = preprocess_name(names_dict, split_list_2[i])\n",
    "    \n",
    "    stitched_name1 = stitch_name(split_list_1)\n",
    "    stitched_name2 = stitch_name(split_list_2)\n",
    "    \n",
    "    # 1st layer of testing: Token Sort Ratio with threshold\n",
    "    score1 = fuzz.token_sort_ratio(stitched_name1, stitched_name2)\n",
    "    if score1 >= threshold:\n",
    "        # score_list.append(score1)\n",
    "        return score1\n",
    "        # do something\n",
    "# 4) 2nd layer of testing - Metaphone and NYSIIS phonetic encoding - DONE\n",
    "    else: \n",
    "        matched_phonetic = phonetic_comparison(split_list_1, split_list_2)\n",
    "        if matched_phonetic:\n",
    "            return threshold # assumption that phonetic match will give threshold score\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        return score1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nationality Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hlpr func: get country by cities, states name\n",
    "def get_country(gpe):\n",
    "    geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "    location = geolocator.geocode(gpe)\n",
    "    if location:\n",
    "        loc_lst = location.address.split(',')\n",
    "        return loc_lst[-1]\n",
    "    return None\n",
    "\n",
    "# hlpr func: country dict that returns a list of countries names\n",
    "def countries():\n",
    "    return list(map(lambda x: x.name, list(pycountry.countries)))\n",
    "\n",
    "# hlpr func: return True if name countains country name\n",
    "def contain_country(word, ctry_lst):\n",
    "    for ctry in ctry_lst:\n",
    "        if ctry.lower() in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# hlpr func: extract entities with tag 'GPE', 'ORG', 'NORP'\n",
    "def search_target_ent(tags):\n",
    "    country_lst = countries()\n",
    "    tag_lst = []\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i][1] == 'GPE' or tags[i][1] == 'ORG' or tags[i][1] == 'NORP':\n",
    "            if contain_country(tags[i][0], country_lst):\n",
    "                tag_lst.append(tags[i])\n",
    "    return tag_lst\n",
    "\n",
    "# hlpr func: calculation for nationality matching score\n",
    "def calc_odd_nationality(nat,lst):\n",
    "    try:\n",
    "        result = []\n",
    "        for tag in lst:\n",
    "            if tag[0] is not None and nat is not None:\n",
    "                if nat.lower() in tag[0].lower():\n",
    "                    result.append(tag)\n",
    "                    continue\n",
    "            try:\n",
    "                if tag[1] == 'GPE' and (get_country(tag[0]) is not None and nat is not None):\n",
    "                    if get_country(tag[0]).lower() == nat.lower():\n",
    "                        result.append(tag)\n",
    "            except GeocoderServiceError as e:\n",
    "                pass\n",
    "        prob = 1 if ((len(lst) - len(result)) == 0 and len(result) > 0) else (len(result) / (len(lst) - len(result)))\n",
    "        prob = 1 if prob > 1 else prob\n",
    "        return prob\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "\n",
    "# hlpr func: return True if name fuzzy matching score > 80\n",
    "def is_target(name, article_name):\n",
    "    return fuzz.partial_ratio(name, article_name) > 80\n",
    "\n",
    "# main function\n",
    "# Params: tags: tokenised text, nationality, person: person's name\n",
    "# Return: nationality matching score\n",
    "def nationality_matching(tags, nationality, person):\n",
    "    \n",
    "    if nationality is None:\n",
    "        return None\n",
    "    \n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "            if tags[i][1] == 'PERSON':\n",
    "                \n",
    "                # check if is target\n",
    "                if is_target(person, tags[i][0]):\n",
    "                    search = search_target_ent(tags)\n",
    "                \n",
    "                    if len(search) != 0:\n",
    "                        return calc_odd_nationality(nationality, search)\n",
    "        return 0\n",
    "    except IndexError as e:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hlpr func: tokenise text to tags\n",
    "def parse(text):\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "# hlpr func: return True if token is a name and subject\n",
    "def is_name_subj(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ == 'nsubjpass')  and token.pos_ == 'PROPN'\n",
    "\n",
    "# hlpr func: return True if token is part of a name\n",
    "def is_part_of_name(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ =='compound' or token.dep_ == 'nsubjpass') \\\n",
    "        and token.pos_ == 'PROPN'\n",
    "\n",
    "# hlpr func: return True if the token is a determiner: his, her, hers\n",
    "def is_det(token):\n",
    "    return token.pos_ == 'DET' and (token.dep_ == 'poss' or token.dep_ == 'attr')\n",
    "\n",
    "# hlpr func: return True if the token is a pronoun: he, she, herself, himself\n",
    "def is_pron(token):\n",
    "    return token.pos_ == 'PRON' and \\\n",
    "        (token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' or token.dep_ == 'pobj' or token.dep_ == 'dobj')\n",
    "\n",
    "# hlpr func: return True if the gender noun is referring to target person\n",
    "def refer_target(gender, noun, name, text):\n",
    "    m = ['man', 'boy', 'guy']\n",
    "    f = ['woman', 'lady', 'girl']\n",
    "    \n",
    "    if is_target(name, text):\n",
    "        return (gender == 'male' and noun in m) or (gender == 'female' and noun in f)\n",
    "    return 0\n",
    "\n",
    "# hlpr func: return True if gender noun is follwed by 'is, was, as or comma'\n",
    "def gender_noun(t1, t2):\n",
    "    gender_nouns = ['man', 'boy', 'guy', 'woman', 'lady', 'girl']\n",
    "    verbs = ['was', 'is', 'as', ',']\n",
    "    return (t1 in gender_nouns) and (t2 in verbs)\n",
    "\n",
    "# hlpr func: return the probability of the gender in article to the true gender\n",
    "def calc_prob_gender(pron_lst, gender):\n",
    "    male_pron = ['he', 'his', 'himself', 'him']\n",
    "    female_pron = ['she', 'her', 'herself', 'hers']\n",
    "    n_target = 0\n",
    "    gdr_pron = []\n",
    "    \n",
    "    if gender.lower() == 'male':\n",
    "        gdr_pron = male_pron\n",
    "    else:\n",
    "        gdr_pron = female_pron\n",
    "        \n",
    "    for pron in pron_lst:\n",
    "        if pron in gdr_pron:\n",
    "            n_target += 1\n",
    "    return n_target / len(pron_lst) if len(pron_lst) else 0\n",
    "\n",
    "# main function\n",
    "# Params: text: article's text, gender, name: person's name\n",
    "# Return: gender matching score\n",
    "def gender_matching(text, gender, name):\n",
    "    \n",
    "    if gender is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        pron_lst = ['he', 'his', 'himself', 'him', 'she', 'her', 'herself', 'hers']\n",
    "        name_str = ''\n",
    "        target_name = name.replace(\" \", \"\")\n",
    "        target_found = False\n",
    "        res_lst = []\n",
    "        \n",
    "        # text tagging\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(doc):\n",
    "\n",
    "            # catch text like '...woman is/was/as/, xxx...'\n",
    "            if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                    return (1)\n",
    "\n",
    "            # search for target name of subject form\n",
    "            if is_name_subj(doc[i]):\n",
    "                end_name = i\n",
    "                start_name = i\n",
    "                while is_part_of_name(doc[start_name]):\n",
    "                    start_name -= 1\n",
    "                start_name += 1\n",
    "                while start_name <= end_name:\n",
    "                    name_str += doc[start_name].text\n",
    "                    start_name += 1\n",
    "\n",
    "            if is_target(name_str, target_name):\n",
    "                target_found = True\n",
    "            else:\n",
    "                name_str = ''\n",
    "                target_found = False\n",
    "          \n",
    "            # if target name is found, search for pronouns, break if another name is found\n",
    "            while target_found:\n",
    "                i+=1\n",
    "                if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                    if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                        return (1)\n",
    "                if is_name_subj(doc[i]):\n",
    "                    target_found = False\n",
    "                    name_str = ''\n",
    "                    break\n",
    "                if is_det(doc[i]) or is_pron(doc[i]):\n",
    "                    if (doc[i].text).lower() in pron_lst:\n",
    "                        res_lst.append((doc[i].text).lower())\n",
    "                        break\n",
    "\n",
    "            i += 1\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return calc_prob_gender(res_lst, gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_dates = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','yesterday','today']\n",
    "\n",
    "\n",
    "def forward_searcher(index,tags):\n",
    "    '''\n",
    "    index: index of subject within tag\n",
    "    tags: all tags extracted from text\n",
    "    searches forwards for DATE tags nearest to index of subject\n",
    "    '''\n",
    "    for i in range(index,len(tags)):\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "    return [None,None]\n",
    "\n",
    "def backward_searcher(index,tags):\n",
    "    '''\n",
    "    index: index of subject within tag\n",
    "    tags: all tags extracted from text\n",
    "    searches backwards for DATE tags nearest to index of subject\n",
    "    '''\n",
    "    \n",
    "    i = index\n",
    "    while i >= 0:\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "        else:\n",
    "            i -=1\n",
    "\n",
    "def detect_age(age,lst):\n",
    "    '''\n",
    "    detects age within the list of [forwards search, backwards search]\n",
    "    '''\n",
    "    try:\n",
    "        if lst[1] is not None and lst[2] is not None:\n",
    "            date1 = lst[1][0]\n",
    "            date2 = lst[2][0]\n",
    "            if (str(age) in date1) or (str(age) in date2):\n",
    "                return True\n",
    "        else:\n",
    "\n",
    "            if lst[1] == None:\n",
    "                if str(age) in lst[2][0]:\n",
    "                    return True\n",
    "\n",
    "            if lst[2] == None:\n",
    "                if str(age) in lst[1][0]:\n",
    "                    return True\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def confirm_age(lst,age,threshold):\n",
    "    '''\n",
    "    confirms the age within the threshold\n",
    "    '''\n",
    "    iterating_lst = []\n",
    "    plus = 1\n",
    "    minus = -1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+plus)\n",
    "        plus += 1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+minus)\n",
    "        minus -=1 \n",
    "    iterating_lst.append(age)\n",
    "    \n",
    "    for j in iterating_lst:\n",
    "        if str(j) in lst[1][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def age_matching(name_dict,tags,age):\n",
    "    '''\n",
    "    consolidated age matching to run whole age matching algo\n",
    "    tags: parse(text)\n",
    "    age: desired age to check\n",
    "    '''\n",
    "    if age is None:\n",
    "        return None\n",
    "    \n",
    "    for tag in tags:\n",
    "        #if tag[1] == 'DATE':\n",
    "            #print(tag)\n",
    "        if str(age) in tag[0] or str(age+1) in tag[0] or str(age-1) in tag[0]:\n",
    "            return 1\n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "\n",
    "            \n",
    "            if tags[i][1] == 'PERSON':\n",
    "                if tags[i][0] in name_dict:\n",
    "\n",
    "                    forward_age = forward_searcher(i,tags)\n",
    "                    backwards_age = backward_searcher(i,tags)\n",
    "                    new_list = [tags[i],forward_age,backwards_age]\n",
    "                    #new_list = [tags[i-1],tags[i],tags[i+1]]\n",
    "                    #print(new_list)\n",
    "\n",
    "                    if detect_age(age,new_list) and tags[i][0] in name_list:\n",
    "\n",
    "                        #print(new_list)\n",
    "                        #result += new_list\n",
    "\n",
    "                        if str(age) in new_list[1][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i], new_list[1]])\n",
    "                            return(confirm_age([tags[i],new_list[1]],age,3))\n",
    "\n",
    "\n",
    "                        elif str(age) in new_list[2][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i],new_list[2]])\n",
    "                            return(confirm_age([tags[i],new_list[2]],age,3))\n",
    "                        \n",
    "        return 0\n",
    "    except IndexError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "# Params: input_info: individual dictionary, text: articls's test, names_list\n",
    "# Return: final confidence score for each article\n",
    "def entity_recognition_scoring_each_article(input_info, text, names_list):\n",
    "    output = []\n",
    "    input_name = input_info['name']\n",
    "\n",
    "\n",
    "    article_names_list = names_list.most_common() \n",
    "    matched = False\n",
    "\n",
    "    for each_name, each_count in article_names_list: ## as of now checking all names within the article, should we limit to e.g. top 3/5?\n",
    "        if len(each_name.split()) == 1 and each_name in input_name:\n",
    "            score = 100 ## if surname matches, default match score 100 \n",
    "        else: \n",
    "            try: \n",
    "                score = ER_name_matching(input_name, each_name)\n",
    "            except ValueError as e:\n",
    "                score = None\n",
    "        if score is not None:\n",
    "            matched = True\n",
    "        if matched:\n",
    "            break\n",
    "    conf_score = 0\n",
    "    if matched:\n",
    "        name_score = score\n",
    "        nationality_score = nationality_matching(parse(text), input_info['nationality'], input_info['name'])\n",
    "        gender_score = gender_matching(text, input_info['gender'], input_info['name'])\n",
    "        age_score = age_matching(names_list,parse(text),input_info['year_of_birth'])\n",
    "        \n",
    "        denom = 0.9071\n",
    "        if nationality_score is not None:\n",
    "            denom += 0.049973\n",
    "        if gender_score is not None:\n",
    "            denom += 0.030293\n",
    "        if age_score is not None:\n",
    "            denom += 0.012634\n",
    "\n",
    "        conf_score = ((0.9071 / denom) * (name_score/100))\n",
    "\n",
    "        if nationality_score is not None:\n",
    "            conf_score += ((0.049973 / denom) * nationality_score)\n",
    "        if gender_score is not None:\n",
    "            conf_score += ((0.030293 / denom) * gender_score)\n",
    "        if age_score is not None:\n",
    "            conf_score += ((0.012634 / denom) * age_score)\n",
    "                           \n",
    "    return conf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def search_articles_on_individual(individual_dict, no_of_articles=30):\n",
    "    '''\n",
    "    individual dict: dictionary of variables pulled directly from excel\n",
    "    '''\n",
    "    def generate_link(person_dict, attributes_used = ['name'], keywords=['crimes', 'sentenced']):\n",
    "\n",
    "        link_start = \"https://www.google.com/search?q=\"\n",
    "        link_end = \"&sxsrf=ALeKk01K1bOuJFHjy4HBARo1cRpUYakYPg:1629640327633&source=lnms&tbm=nws&sa=X&sqi=2&ved=2ahUKEwiu29um48TyAhWGqpUCHYuoAlcQ_AUoAnoECAEQBA&biw=1441&bih=718&dpr=2\" \n",
    "        link_query = \"\"\n",
    "\n",
    "        for attributes in attributes_used:\n",
    "            temp_attr = person_dict[attributes]\n",
    "            if temp_attr is not None:\n",
    "                temp_attr = str(temp_attr)\n",
    "                link_query += temp_attr.replace(' ', '+') + '+'       \n",
    "                \n",
    "        links = []\n",
    "        for keyword in keywords:\n",
    "            temp_search_link = link_start + link_query + keyword + link_end + \"&num=\" + str(no_of_articles)\n",
    "            links.append(temp_search_link)\n",
    "        return links\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_link_non_news(person_dict, attributes_used = ['name'], keywords=['crimes', 'sentenced']):\n",
    "        link_start = \"https://www.google.com/search?q=\"\n",
    "        link_end = '&sxsrf=AOaemvJeyEECa-gYsQeldYm25EieK_KRpQ:1631766265071&source=lnms&sa=X&ved=2ahUKEwiYuNeE04LzAhXhV3wKHclPC0IQ_AUoAHoECAEQAg&biw=1291&bih=643&dpr=2.2'\n",
    "        link_query = \"\"\n",
    "        \n",
    "        for attributes in attributes_used:\n",
    "            temp_attr = person_dict[attributes]\n",
    "            if temp_attr is not None:\n",
    "                temp_attr = str(temp_attr)\n",
    "                link_query += temp_attr.replace(' ', '+') + '+'       \n",
    "                \n",
    "        links = []\n",
    "        for keyword in keywords:\n",
    "            temp_search_link = link_start + link_query + keyword + link_end + \"&num=\" + str(no_of_articles)\n",
    "            links.append(temp_search_link)\n",
    "        return links\n",
    "    \n",
    "    def article_extraction(link):\n",
    "        '''\n",
    "        extract text from url of link\n",
    "        '''\n",
    "        article = Article(link)\n",
    "        article.download()\n",
    "        try:\n",
    "            article.parse()\n",
    "        except:\n",
    "            pass\n",
    "        return article.text\n",
    "\n",
    "    def parse(text):\n",
    "        '''\n",
    "        transform text into NLP tags\n",
    "        '''\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def find_names(tags):\n",
    "        '''\n",
    "        extract PERSON tags from text\n",
    "        '''\n",
    "        names = []\n",
    "        for tag in tags:\n",
    "            if tag[1] == 'PERSON':\n",
    "                names.append(tag[0])\n",
    "        return names\n",
    "    \n",
    "    def time_to_months(time):\n",
    "        '''\n",
    "        given time time since the article is published, extract the months since the article was published.\n",
    "        '''\n",
    "        if 'weeks' in time:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(time.split(' month')[0])\n",
    "\n",
    "    search_links = generate_link(individual_dict)\n",
    "    \n",
    "    unique_links_checker = []\n",
    "    \n",
    "    output = []\n",
    "    for x in search_links:\n",
    "        req = Request(x, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "        webpage = urlopen(req).read()\n",
    "\n",
    "        with requests.Session() as c:\n",
    "            soup = BeautifulSoup(webpage, 'html5lib')\n",
    "            #print(soup)\n",
    "            for item in soup.find_all('div', attrs = {'class': \"ZINbbc xpd O9g5cc uUPGi\"}):\n",
    "                current_dict = {}\n",
    "                raw_link = (item.find('a', href = True)['href'])\n",
    "                try:\n",
    "                    link = (raw_link.split(\"/url?q=\")[1]).split('&sa=U&')[0]\n",
    "                except IndexError as e1:\n",
    "                    continue\n",
    "                if link not in unique_links_checker and item:\n",
    "                    unique_links_checker.append(link)\n",
    "                    title = item.find('div',attrs = {'class': 'BNeawe vvjwJb AP7Wnd'})\n",
    "                    if title == None:\n",
    "                        continue\n",
    "                    title = title.get_text()\n",
    "                    description  = (item.find('div',attrs = {'class': 'BNeawe s3v9rd AP7Wnd'}).get_text())\n",
    "                    time = description.split(\" · \")[0]\n",
    "                    #print(description)\n",
    "                    \n",
    "                    try:\n",
    "                        descript = description.split(\" · \")[1]\n",
    "\n",
    "                        # create names_list\n",
    "                        parsed_text = parse(article_extraction(link))\n",
    "                        names_in_text = find_names(parsed_text)\n",
    "                        names_list = Counter(names_in_text)\n",
    "                    except IndexError as e2:\n",
    "                        \n",
    "                        pass\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    # extract text\n",
    "                    text = article_extraction(link)\n",
    "\n",
    "                    # compute confidence score before accepting the article\n",
    "                    conf_score = entity_recognition_scoring_each_article(individual_dict, text, names_list)\n",
    "                    \n",
    "                    # this is the new part 0.9071\n",
    "                    overall_threshold = 0.8\n",
    "                    \n",
    "                    nationality = individual_dict['nationality']\n",
    "                    gender = individual_dict['gender']\n",
    "                    year_of_birth = individual_dict['year_of_birth']\n",
    "                    \n",
    "                    if nationality is not None:\n",
    "                        overall_threshold += 0.049973\n",
    "                    if gender is not None:\n",
    "                        overall_threshold += 0.030293\n",
    "                    if year_of_birth is not None:\n",
    "                        overall_threshold += 0.012634\n",
    "                  \n",
    "\n",
    "                    if conf_score < overall_threshold:\n",
    "                        continue\n",
    "            \n",
    "                    current_dict['title'] = title\n",
    "                    current_dict['time'] = time\n",
    "                    try:\n",
    "                        current_dict['year_of_birth'] = (date.today() - relativedelta(months = time_to_months(time))).year - individual_dict['year_of_birth']\n",
    "                    except TypeError as e1:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    except ValueError as e2:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    current_dict['description'] = descript\n",
    "                    current_dict['link'] = link\n",
    "                    current_dict['text'] = text\n",
    "                    current_dict['names_list'] = names_list\n",
    "                    current_dict['confidence_score'] = conf_score\n",
    "                    \n",
    "                    #check for ratio of confidence score that fails\n",
    "                    \n",
    "                    \n",
    "                    output.append(current_dict)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    count_unsatisfactory = 0    \n",
    "    for row in output:\n",
    "        print(row['confidence_score'])\n",
    "        if row['confidence_score'] <= 0.95:\n",
    "            count_unsatisfactory += 1\n",
    "                \n",
    "    print(f\"number of unsatisfactory rows is {count_unsatisfactory}/{len(output)}\")\n",
    "            \n",
    "    #####check for ratio of unsatisfactory to satisfactory#####\n",
    "    #####change this once we confirm the ratio#######\n",
    "    #####currently minimum one output is required#######\n",
    "    if len(output) <= 1 or count_unsatisfactory/len(output) >= 0.7:\n",
    "        print(\"Google News Search has returned unsatisfactory results. Switching to regular google search\")\n",
    "        \n",
    "        search_links = generate_link_non_news(individual_dict)\n",
    "    \n",
    "        unique_links_checker = []\n",
    "\n",
    "        output = []\n",
    "        for x in search_links:\n",
    "            req = Request(x, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "            webpage = urlopen(req).read()\n",
    "\n",
    "            with requests.Session() as c:\n",
    "                soup = BeautifulSoup(webpage, 'html5lib')\n",
    "                #print(soup)\n",
    "                for item in soup.find_all('div', attrs = {'class': \"ZINbbc xpd O9g5cc uUPGi\"}):\n",
    "                    current_dict = {}\n",
    "                    raw_link = (item.find('a', href = True)['href'])\n",
    "                    try:\n",
    "                        link = (raw_link.split(\"/url?q=\")[1]).split('&sa=U&')[0]\n",
    "                    except IndexError as e1:\n",
    "                        continue\n",
    "                    if link not in unique_links_checker and item:\n",
    "                        unique_links_checker.append(link)\n",
    "                        title = item.find('div',attrs = {'class': 'BNeawe vvjwJb AP7Wnd'})\n",
    "                        if title == None:\n",
    "                            continue\n",
    "                        title = title.get_text()\n",
    "                        description  = (item.find('div',attrs = {'class': 'BNeawe s3v9rd AP7Wnd'}).get_text())\n",
    "                        time = description.split(\" · \")[0]\n",
    "                        #print(description)\n",
    "                        \n",
    "                        try:\n",
    "                            descript = description.split(\" · \")[1]\n",
    "\n",
    "                            # create names_list\n",
    "                            parsed_text = parse(article_extraction(link))\n",
    "                            names_in_text = find_names(parsed_text)\n",
    "                            names_list = Counter(names_in_text)\n",
    "                            \n",
    "                        except IndexError as e2:\n",
    "                            descript = \"No Description\"\n",
    "                            \n",
    "                            parsed_text = parse(article_extraction(link))\n",
    "                            names_in_text = find_names(parsed_text)\n",
    "                            names_list = Counter(names_in_text)\n",
    "                            \n",
    "\n",
    "                        # extract text\n",
    "                        text = article_extraction(link)\n",
    "\n",
    "                        # compute confidence score before accepting the article\n",
    "                        conf_score = entity_recognition_scoring_each_article(individual_dict, text, names_list)\n",
    "\n",
    "                        # this is the new part 0.9071\n",
    "                        overall_threshold = 0.8\n",
    "\n",
    "                        nationality = individual_dict['nationality']\n",
    "                        gender = individual_dict['gender']\n",
    "                        year_of_birth = individual_dict['year_of_birth']\n",
    "\n",
    "                        if nationality is not None:\n",
    "                            overall_threshold += 0.049973\n",
    "                        if gender is not None:\n",
    "                            overall_threshold += 0.030293\n",
    "                        if year_of_birth is not None:\n",
    "                            overall_threshold += 0.012634\n",
    "\n",
    "\n",
    "                        if conf_score < overall_threshold:\n",
    "                            continue\n",
    "\n",
    "                        current_dict['title'] = title\n",
    "                        current_dict['time'] = time\n",
    "                        try:\n",
    "                            current_dict['year_of_birth'] = (date.today() - relativedelta(months = time_to_months(time))).year - individual_dict['year_of_birth']\n",
    "                        except TypeError as e1:\n",
    "                            current_dict['year_of_birth'] = 0\n",
    "                        except ValueError as e2:\n",
    "                            current_dict['year_of_birth'] = 0\n",
    "                        current_dict['description'] = descript\n",
    "                        current_dict['link'] = link\n",
    "                        current_dict['text'] = text\n",
    "                        current_dict['names_list'] = names_list\n",
    "                        current_dict['confidence_score'] = conf_score\n",
    "\n",
    "                        #check for ratio of confidence score that fails\n",
    "\n",
    "\n",
    "                        output.append(current_dict)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def sentiment_model(name_matched):\n",
    "    \n",
    "    if len(name_matched) == 0:\n",
    "        sys.exit(\"No Articles found.\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # Loading Model\n",
    "        reconstructed_model = keras.models.load_model(\"LSTM_GLOVE\")\n",
    "\n",
    "        url = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
    "        (?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([\n",
    "          ^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        def clean_data(temp):\n",
    "            temp = temp.map(lambda x:str(x).lower()) \n",
    "            # removing emails\n",
    "            temp = temp.map(lambda x:re.sub(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\", \"\", x)) \n",
    "            # removing url\n",
    "            temp = temp.map(lambda x:re.sub(url, \"\", x)) \n",
    "            # removing numbers\n",
    "            temp = temp.map(lambda x:re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \"\", x)) \n",
    "            # removing white space\n",
    "            temp = temp.map(lambda x:re.sub(r'^\\s*|\\s\\s*', ' ', x).strip()) \n",
    "            # removing punctuations\n",
    "            temp = temp.map(lambda x:''.join([c for c in x if c not in string.punctuation])) \n",
    "            # removing special characters\n",
    "            temp = temp.map(lambda x:re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', x)) \n",
    "            # unicode\n",
    "            temp = temp.map(lambda x:unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')) \n",
    "            # tokenising text for cleaning\n",
    "            temp = temp.map(lambda x:tokenizer.tokenize(x)) \n",
    "            # removing stop words\n",
    "            temp = temp.map(lambda x:[i for i in x if i not in stopwords.words('english')]) \n",
    "            temp = temp.map(lambda x:' '.join(x))\n",
    "            return temp\n",
    "\n",
    "        name_matched['body'] = name_matched['text']\n",
    "        name_matched.text = clean_data(name_matched.text)\n",
    "\n",
    "        # Data Preprocessing for model ingestion\n",
    "        maxlen = 50\n",
    "        embedding_dim = 100\n",
    "\n",
    "        X = name_matched.text.values\n",
    "        tokenizer = Tokenizer(num_words=5000)\n",
    "        tokenizer.fit_on_texts(name_matched.text.values)\n",
    "        X = tokenizer.texts_to_sequences(X)\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        test_input = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
    "\n",
    "        # Predicting output\n",
    "        test = reconstructed_model.predict(test_input)\n",
    "        test_classes = np.argmax(test,axis=1)\n",
    "        name_matched['prediction'] = test_classes\n",
    "        \n",
    "        def predicted_classes(df):\n",
    "            val = ''  \n",
    "            if df['prediction'] == 2:\n",
    "                val = 'negative'\n",
    "            elif df['prediction'] == 0:\n",
    "                val = 'neutral'\n",
    "            else:\n",
    "                val = 'positive'\n",
    "\n",
    "            return val\n",
    "            \n",
    "\n",
    "        name_matched['sentiment'] = name_matched.apply(predicted_classes,axis=1)\n",
    "        \n",
    "        name_matched = name_matched[['title', 'time', 'year_of_birth', 'description', 'link', 'body',\n",
    "                                       'names_list', 'confidence_score']]#, 'sentiment']]\n",
    "    \n",
    "        return name_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo (Driver function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(input):\n",
    "    # Web Scraping\n",
    "    df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    df_dict = preprocess_df_to_dict(df)\n",
    "    test_record_1 = df_dict[input]\n",
    "    \n",
    "    print('Test input: \\n')\n",
    "    print('Name: ' + str(test_record_1['actual_name']))\n",
    "    print('Alias: ' + str(test_record_1['alias']))\n",
    "    print('Year of Birth: ' + str(test_record_1['year_of_birth']))\n",
    "    print('Month of Birth: ' + str(test_record_1['month_of_birth']))\n",
    "    print('Day of Birth: ' + str(test_record_1['day_of_birth']))\n",
    "    print('Gender: ' + str(test_record_1['gender']))\n",
    "    print('Nationality: ' + str(test_record_1['nationality']))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    test_query = search_articles_on_individual(test_record_1, 10)\n",
    "    test_query = pd.DataFrame(test_query)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    output = sentiment_model(test_query)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individuals associated with Financial Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Son Mun San',\n",
       " 'alias': None,\n",
       " 'year_of_birth': 1951,\n",
       " 'month_of_birth': None,\n",
       " 'day_of_birth': None,\n",
       " 'gender': '-',\n",
       " 'nationality': None,\n",
       " 'type_of_error': 'Transposed characters',\n",
       " 'actual_name': 'Son Mun San'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df_dict = preprocess_df_to_dict(df)\n",
    "test_record_1 = df_dict[5]\n",
    "test_record_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \n",
      "\n",
      "Name: Son Mun San\n",
      "Alias: None\n",
      "Year of Birth: 1951\n",
      "Month of Birth: None\n",
      "Day of Birth: None\n",
      "Gender: -\n",
      "Nationality: None\n",
      "\n",
      "\n",
      "0.9601293787790591\n",
      "number of unsatisfactory rows is 0/1\n",
      "Google News Search has returned unsatisfactory results. Switching to regular google search\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9eb13a2b73c5>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Sentiment Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9eb13a2b73c5>\u001b[0m in \u001b[0;36msentiment_model\u001b[0;34m(name_matched)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;31m# Loading Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mreconstructed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LSTM_GLOVE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         url = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m   model = tf_load.load_internal(\n\u001b[0m\u001b[1;32m    121\u001b[0m       path, options=options, loader_cls=KerasObjectLoader)\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m    633\u001b[0m                             ckpt_options)\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options)\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WrapperFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# loaded from config may create variables / other objects during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# initialization. These are recorded in `_nodes_recreated_from_config`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Load all other nodes and functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m       \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_load_layer\u001b[0;34m(self, proto, node_id)\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;34m\"\"\"Load a single layer from a SavedUserObject proto.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# If node was already created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_decode_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 4 Tay Sheng Yang: Singaporean involved fraudulent cashback schemes that cheated Spring Singapore and the WDA\n",
    "# 3 Ng Yu Zhi: The former director of Envy Global Trading - Charged with running the Singapore largest Ponzi scheme\n",
    "\n",
    "demo(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individuals associated with Non-Financial Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individuals with no adverse news associated to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a41f48461941a5aefc82610e94371dafd9500cffa630c3ca9566e477af78c3ed"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
